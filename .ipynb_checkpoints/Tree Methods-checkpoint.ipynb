{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c60f3d7d",
   "metadata": {},
   "source": [
    "### Tree Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a5cc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/12 22:09:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/12 22:09:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/12 22:09:41 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "# Must be included at the beginning of each new notebook. Remember to change the app name.\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('722TreeMethods').getOrCreate()\n",
    "\n",
    "# import other packages\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import (StructField,StringType,IntegerType,StructType)\n",
    "\n",
    "# Import VectorAssembler and Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c748c5b",
   "metadata": {},
   "source": [
    " Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84eceec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate the 2 sources\n",
    "# code adapted from https://www.geeksforgeeks.org/merge-two-dataframes-in-pyspark/\n",
    "import functools \n",
    "def unionAll(dfs):\n",
    "    return functools.reduce(lambda df1, df2: df1.union(df2.select(df1.columns)), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "068f5b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable with the correct structure\n",
    "data_schema = [StructField('Total Household Income',IntegerType(),True), StructField('Region',StringType(),True),\n",
    "              StructField('Total Food Expenditure',IntegerType(),True), StructField('Main Source of Income',StringType(),True),\n",
    "              StructField('Agricultural Household indicator',IntegerType(),True), StructField('Bread and Cereals Expenditure',IntegerType(),True),\n",
    "              StructField('Total Rice Expenditure',IntegerType(),True), StructField('Meat Expenditure',IntegerType(),True),\n",
    "              StructField('Total Fish and  marine products Expenditure',IntegerType(),True), StructField('Fruit Expenditure',IntegerType(),True),\n",
    "              StructField('Vegetables Expenditure',IntegerType(),True), StructField('Restaurant and hotels Expenditure',IntegerType(),True),\n",
    "              StructField('Alcoholic Beverages Expenditure',IntegerType(),True), StructField('Tobacco Expenditure',IntegerType(),True),\n",
    "              StructField('Clothing, Footwear and Other Wear Expenditure',IntegerType(),True), StructField('Housing and water Expenditure',IntegerType(),True),\n",
    "              StructField('Imputed House Rental Value',IntegerType(),True), StructField('Medical Care Expenditure',IntegerType(),True),\n",
    "              StructField('Transportation Expenditure',IntegerType(),True), StructField('Communication Expenditure',IntegerType(),True),\n",
    "              StructField('Education Expenditure',IntegerType(),True), StructField('Miscellaneous Goods and Services Expenditure',IntegerType(),True),\n",
    "              StructField('Special Occasions Expenditure',IntegerType(),True), StructField('Crop Farming and Gardening expenses',IntegerType(),True),\n",
    "              StructField('Total Income from Entrepreneurial Acitivites',IntegerType(),True), StructField('Household Head Sex',StringType(),True),\n",
    "              StructField('Household Head Age',IntegerType(),True), StructField('Household Head Marital Status',StringType(),True),\n",
    "              StructField('Household Head Highest Grade Completed',StringType(),True), StructField('Household Head Job or Business Indicator',StringType(),True),\n",
    "              StructField('Household Head Occupation',StringType(),True), StructField('Household Head Class of Worker',StringType(),True),\n",
    "              StructField('Type of Household',StringType(),True), StructField('Total Number of Family members',IntegerType(),True),\n",
    "              StructField('Members with age less than 5 year old',IntegerType(),True), StructField('Members with age 5 - 17 years old',IntegerType(),True),\n",
    "              StructField('Total number of family members employed',IntegerType(),True), StructField('Type of Building/House',StringType(),True),\n",
    "              StructField('Type of Roof',StringType(),True), StructField('Type of Walls',StringType(),True),\n",
    "              StructField('House Floor Area',IntegerType(),True), StructField('House Age',IntegerType(),True),\n",
    "              StructField('Number of bedrooms',IntegerType(),True), StructField('Tenure Status',StringType(),True),\n",
    "              StructField('Toilet Facilities',StringType(),True), StructField('Electricity',IntegerType(),True),\n",
    "              StructField('Main Source of Water Supply',StringType(),True), StructField('Number of Television',IntegerType(),True),\n",
    "              StructField('Number of CD/VCD/DVD',IntegerType(),True), StructField('Number of Component/Stereo set',IntegerType(),True),\n",
    "              StructField('Number of Refrigerator/Freezer',IntegerType(),True), StructField('Number of Washing Machine',IntegerType(),True),\n",
    "              StructField('Number of Airconditioner',IntegerType(),True), StructField('Number of Car, Jeep, Van',IntegerType(),True),\n",
    "              StructField('Number of Landline/wireless telephones',IntegerType(),True), StructField('Number of Cellular phone',IntegerType(),True),\n",
    "              StructField('Number of Personal Computer',IntegerType(),True), StructField('Number of Stove with Oven/Gas Range',IntegerType(),True),\n",
    "              StructField('Number of Motorized Banca',IntegerType(),True), StructField('Number of Motorcycle/Tricycle',IntegerType(),True)\n",
    "              ]\n",
    "\n",
    "final_struct = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb45bc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 2) / 3]\r",
      "\r",
      "[Stage 0:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset with updated dtypes: 41544 x 60\n",
      "root\n",
      " |-- Total Household Income: integer (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Total Food Expenditure: integer (nullable = true)\n",
      " |-- Main Source of Income: string (nullable = true)\n",
      " |-- Agricultural Household indicator: integer (nullable = true)\n",
      " |-- Bread and Cereals Expenditure: integer (nullable = true)\n",
      " |-- Total Rice Expenditure: integer (nullable = true)\n",
      " |-- Meat Expenditure: integer (nullable = true)\n",
      " |-- Total Fish and  marine products Expenditure: integer (nullable = true)\n",
      " |-- Fruit Expenditure: integer (nullable = true)\n",
      " |-- Vegetables Expenditure: integer (nullable = true)\n",
      " |-- Restaurant and hotels Expenditure: integer (nullable = true)\n",
      " |-- Alcoholic Beverages Expenditure: integer (nullable = true)\n",
      " |-- Tobacco Expenditure: integer (nullable = true)\n",
      " |-- Clothing, Footwear and Other Wear Expenditure: integer (nullable = true)\n",
      " |-- Housing and water Expenditure: integer (nullable = true)\n",
      " |-- Imputed House Rental Value: integer (nullable = true)\n",
      " |-- Medical Care Expenditure: integer (nullable = true)\n",
      " |-- Transportation Expenditure: integer (nullable = true)\n",
      " |-- Communication Expenditure: integer (nullable = true)\n",
      " |-- Education Expenditure: integer (nullable = true)\n",
      " |-- Miscellaneous Goods and Services Expenditure: integer (nullable = true)\n",
      " |-- Special Occasions Expenditure: integer (nullable = true)\n",
      " |-- Crop Farming and Gardening expenses: integer (nullable = true)\n",
      " |-- Total Income from Entrepreneurial Acitivites: integer (nullable = true)\n",
      " |-- Household Head Sex: string (nullable = true)\n",
      " |-- Household Head Age: integer (nullable = true)\n",
      " |-- Household Head Marital Status: string (nullable = true)\n",
      " |-- Household Head Highest Grade Completed: string (nullable = true)\n",
      " |-- Household Head Job or Business Indicator: string (nullable = true)\n",
      " |-- Household Head Occupation: string (nullable = true)\n",
      " |-- Household Head Class of Worker: string (nullable = true)\n",
      " |-- Type of Household: string (nullable = true)\n",
      " |-- Total Number of Family members: integer (nullable = true)\n",
      " |-- Members with age less than 5 year old: integer (nullable = true)\n",
      " |-- Members with age 5 - 17 years old: integer (nullable = true)\n",
      " |-- Total number of family members employed: integer (nullable = true)\n",
      " |-- Type of Building/House: string (nullable = true)\n",
      " |-- Type of Roof: string (nullable = true)\n",
      " |-- Type of Walls: string (nullable = true)\n",
      " |-- House Floor Area: integer (nullable = true)\n",
      " |-- House Age: integer (nullable = true)\n",
      " |-- Number of bedrooms: integer (nullable = true)\n",
      " |-- Tenure Status: string (nullable = true)\n",
      " |-- Toilet Facilities: string (nullable = true)\n",
      " |-- Electricity: integer (nullable = true)\n",
      " |-- Main Source of Water Supply: string (nullable = true)\n",
      " |-- Number of Television: integer (nullable = true)\n",
      " |-- Number of CD/VCD/DVD: integer (nullable = true)\n",
      " |-- Number of Component/Stereo set: integer (nullable = true)\n",
      " |-- Number of Refrigerator/Freezer: integer (nullable = true)\n",
      " |-- Number of Washing Machine: integer (nullable = true)\n",
      " |-- Number of Airconditioner: integer (nullable = true)\n",
      " |-- Number of Car, Jeep, Van: integer (nullable = true)\n",
      " |-- Number of Landline/wireless telephones: integer (nullable = true)\n",
      " |-- Number of Cellular phone: integer (nullable = true)\n",
      " |-- Number of Personal Computer: integer (nullable = true)\n",
      " |-- Number of Stove with Oven/Gas Range: integer (nullable = true)\n",
      " |-- Number of Motorized Banca: integer (nullable = true)\n",
      " |-- Number of Motorcycle/Tricycle: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# And now we can read in the data using that schema. The other fields are read as integer. \n",
    "df1 = spark.read.option(\"header\", True).csv('dataset/data_ncr.csv', schema=final_struct)\n",
    "df2 = spark.read.option(\"header\", True).csv('dataset/data_therest.csv', schema=final_struct)\n",
    "statsdata = unionAll([df1, df2])\n",
    "\n",
    "print(\"Combined dataset with updated dtypes:\", statsdata.count(), \"x\", len(statsdata.columns))\n",
    "statsdata.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d3f78",
   "metadata": {},
   "source": [
    "Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9c1140d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dimensions after removing 32 columns: 41544 x 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Total Household Income: integer (nullable = true)\n",
      " |-- Main Source of Income: string (nullable = false)\n",
      " |-- Agricultural Household indicator: integer (nullable = true)\n",
      " |-- Imputed House Rental Value: integer (nullable = true)\n",
      " |-- Medical Care Expenditure: integer (nullable = true)\n",
      " |-- Total Income from Entrepreneurial Activities: integer (nullable = true)\n",
      " |-- Household Head Sex: string (nullable = false)\n",
      " |-- Household Head Age: integer (nullable = true)\n",
      " |-- Household Head Marital Status: string (nullable = false)\n",
      " |-- Household Head Highest Grade Completed: string (nullable = false)\n",
      " |-- Household Head Job or Business Indicator: string (nullable = false)\n",
      " |-- Household Head Occupation: string (nullable = false)\n",
      " |-- Household Head Class of Worker: string (nullable = false)\n",
      " |-- Type of Household: string (nullable = false)\n",
      " |-- Total Number of Family members: integer (nullable = true)\n",
      " |-- Members with age less than 5 year old: integer (nullable = true)\n",
      " |-- Members with age 5 - 17 years old: integer (nullable = true)\n",
      " |-- Total number of family members employed: integer (nullable = true)\n",
      " |-- Type of Building/House: string (nullable = false)\n",
      " |-- Type of Roof: string (nullable = false)\n",
      " |-- Type of Walls: string (nullable = false)\n",
      " |-- House Floor Area: integer (nullable = true)\n",
      " |-- House Age: integer (nullable = true)\n",
      " |-- Number of bedrooms: integer (nullable = true)\n",
      " |-- Tenure Status: string (nullable = false)\n",
      " |-- Toilet Facilities: string (nullable = false)\n",
      " |-- Electricity: integer (nullable = true)\n",
      " |-- Main Source of Water Supply: string (nullable = false)\n",
      " |-- Total Expenditures: integer (nullable = true)\n",
      " |-- Main Source of Income Index: double (nullable = false)\n",
      " |-- Household Head Sex Index: double (nullable = false)\n",
      " |-- Household Head Marital Status Index: double (nullable = false)\n",
      " |-- Household Head Highest Grade Completed Index: double (nullable = false)\n",
      " |-- Household Head Job or Business Indicator Index: double (nullable = false)\n",
      " |-- Household Head Occupation Index: double (nullable = false)\n",
      " |-- Household Head Class of Worker Index: double (nullable = false)\n",
      " |-- Type of Household Index: double (nullable = false)\n",
      " |-- Type of Building/House Index: double (nullable = false)\n",
      " |-- Type of Roof Index: double (nullable = false)\n",
      " |-- Type of Walls Index: double (nullable = false)\n",
      " |-- Tenure Status Index: double (nullable = false)\n",
      " |-- Toilet Facilities Index: double (nullable = false)\n",
      " |-- Main Source of Water Supply Index: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop columns\n",
    "Columns_To_Remove = ['Region', 'Number of Television', 'Number of CD/VCD/DVD', 'Number of Component/Stereo set', \n",
    "                     'Number of Refrigerator/Freezer', 'Number of Washing Machine', 'Number of Airconditioner', \n",
    "                     'Number of Car, Jeep, Van', 'Number of Landline/wireless telephones', 'Number of Cellular phone', \n",
    "                     'Number of Personal Computer', 'Number of Stove with Oven/Gas Range', 'Number of Motorized Banca', \n",
    "                     'Number of Motorcycle/Tricycle']\n",
    "statsupdate = statsdata.drop(*Columns_To_Remove)\n",
    "\n",
    "# Add New Column with Total Expenses\n",
    "expenses_cols = ['Total Food Expenditure', 'Bread and Cereals Expenditure', 'Total Rice Expenditure', 'Meat Expenditure', 'Total Fish and  marine products Expenditure', 'Fruit Expenditure', 'Vegetables Expenditure', 'Restaurant and hotels Expenditure', 'Alcoholic Beverages Expenditure', 'Tobacco Expenditure', 'Clothing, Footwear and Other Wear Expenditure', 'Housing and water Expenditure', 'Medical Care Expenditure', 'Transportation Expenditure', 'Communication Expenditure', 'Education Expenditure', 'Miscellaneous Goods and Services Expenditure', 'Special Occasions Expenditure', 'Crop Farming and Gardening expenses']\n",
    "statsupdate = statsupdate.withColumn('Total Expenditures', sum(statsupdate[col] for col in expenses_cols))\n",
    "\n",
    "# Drop the other expenses columns, except Medical Care Expenditure\n",
    "expenses_cols.remove('Medical Care Expenditure')\n",
    "statsupdate = statsupdate.drop(*expenses_cols)\n",
    "print(\"Updated dimensions after removing\",len(expenses_cols)+len(Columns_To_Remove),\"columns:\",statsupdate.count(), \"x\", len(statsupdate.columns))\n",
    "\n",
    "# remove na/nulls\n",
    "statsupdate = statsupdate.na.fill('Not Specified')\n",
    "\n",
    "# remove outliers\n",
    "# calculate upper and lower bounds for outliers, adapted from internet\n",
    "def calculate_bounds(df):\n",
    "    bounds = {\n",
    "        c: dict(\n",
    "            zip([\"q1\", \"q3\"], df.approxQuantile(c, [0.25, 0.75], 0))\n",
    "        )\n",
    "        for c,d in zip(df.columns, df.dtypes) if d[1] == \"int\"\n",
    "    }\n",
    "    for c in bounds:\n",
    "        iqr = bounds[c]['q3'] - bounds[c]['q1']\n",
    "        bounds[c]['min'] = bounds[c]['q1'] - (iqr * 1.5)\n",
    "        bounds[c]['max'] = bounds[c]['q3'] + (iqr * 1.5)\n",
    "    return bounds\n",
    "bounds = calculate_bounds(statsupdate)\n",
    "\n",
    "# remove outliers for \"Total Household Income\"\n",
    "c = \"Total Household Income\"\n",
    "newstats = statsupdate.filter(F.col(c).between(bounds[c]['min'], bounds[c]['max']))\n",
    "\n",
    "# remove outliers for \"Medical Care Expenditure\"\n",
    "c = \"Medical Care Expenditure\"\n",
    "newstats = newstats.filter(F.col(c).between(bounds[c]['min'], bounds[c]['max']))\n",
    "\n",
    "# rename misspelled column\n",
    "newstats = newstats.withColumnRenamed(\"Total Income from Entrepreneurial Acitivites\", \"Total Income from Entrepreneurial Activities\")\n",
    "\n",
    "# categorize\n",
    "#create a list of the columns that are string types\n",
    "categoricalColumns = [item[0] for item in newstats.dtypes if item[1].startswith('string') ]\n",
    "\n",
    "#define a list of stages in your pipeline. The string indexer will be one stage\n",
    "stages = []\n",
    "#iterate through all categorical values, create a string indexer, assign new column name with 'Index' at end\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + ' Index')\n",
    "    stages += [stringIndexer]\n",
    "\n",
    "pipeline = Pipeline(stages = stages)\n",
    "newstats = pipeline.fit(newstats).transform(newstats)\n",
    "\n",
    "newstats.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d44de9d",
   "metadata": {},
   "source": [
    "### Run Tree Methods Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be55f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1e3e87",
   "metadata": {},
   "source": [
    "Prepare Data Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c873b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Total Household Income\", \"Main Source of Income Index\", \n",
    "            \"Agricultural Household indicator\",\"Imputed House Rental Value\",\n",
    "            \"Total Income from Entrepreneurial Activities\",\n",
    "            \"Household Head Age\", \"Total Number of Family members\",\n",
    "            \"Members with age less than 5 year old\",\"Members with age 5 - 17 years old\",\n",
    "            \"Total number of family members employed\", \"House Floor Area\",\n",
    "             \"House Age\",\"Number of bedrooms\",\n",
    "             \"Electricity\",\"Total Expenditures\",\n",
    "             \"Main Source of Income Index\",\"Household Head Sex Index\",\n",
    "             \"Household Head Marital Status Index\",\"Household Head Highest Grade Completed Index\",\n",
    "             \"Household Head Job or Business Indicator Index\",\n",
    "             \"Household Head Class of Worker Index\",\"Type of Household Index\",\n",
    "             \"Type of Building/House Index\",\"Type of Roof Index\",\"Type of Walls Index\",\n",
    "             \"Tenure Status Index\",\"Toilet Facilities Index\",\n",
    "             \"Main Source of Water Supply Index\"],\n",
    "    outputCol=\"features\")\n",
    "output = assembler.transform(newstats)\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol = \"Medical Care Expenditure\", outputCol = \"Medical Care Expenditure Label\")\n",
    "\n",
    "output = labelIndexer.fit(output).transform(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffddafb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's select the two columns we want. Features (which contains vectors), and the predictor.\n",
    "final_data = output.select(\"features\",'Medical Care Expenditure Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9398547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- Medical Care Expenditure Label: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/12 22:10:13 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(features=SparseVector(28, {0: 301900.0, 3: 72000.0, 5: 73.0, 6: 4.0, 9: 1.0, 10: 25.0, 11: 15.0, 12: 2.0, 13: 1.0, 14: 381306.0, 16: 1.0, 17: 1.0, 18: 1.0, 19: 1.0, 20: 2.0, 21: 1.0}), Medical Care Expenditure Label=54.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.printSchema()\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cea8a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bab5d32",
   "metadata": {},
   "source": [
    "Create and Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc42334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all 3 models\n",
    "# Use defaults to make the comparison \"fair\". This simplifies the comparison process.\n",
    "dtc = DecisionTreeClassifier(labelCol='Medical Care Expenditure Label',featuresCol='features', maxBins=46)\n",
    "rfc = RandomForestClassifier(labelCol='Medical Care Expenditure Label',featuresCol='features',  maxBins=46)\n",
    "# gbt = GBTClassifier(labelCol='Medical Care Expenditure Label',featuresCol='features',  maxBins=46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33ff39fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/12 22:10:29 WARN DAGScheduler: Broadcasting large task binary with size 1531.5 KiB\n",
      "22/10/12 22:10:35 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "22/10/12 22:10:40 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "22/10/12 22:10:52 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "22/10/12 22:10:54 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/10/12 22:10:58 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "22/10/12 22:11:00 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/10/12 22:11:04 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "22/10/12 22:11:05 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/10/12 22:11:10 WARN DAGScheduler: Broadcasting large task binary with size 9.5 MiB\n",
      "22/10/12 22:11:12 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/10/12 22:11:16 WARN DAGScheduler: Broadcasting large task binary with size 8.3 MiB\n",
      "22/10/12 22:11:17 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/10/12 22:11:21 WARN DAGScheduler: Broadcasting large task binary with size 10.6 MiB\n",
      "22/10/12 22:11:23 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/10/12 22:11:27 WARN DAGScheduler: Broadcasting large task binary with size 10.6 MiB\n",
      "22/10/12 22:11:29 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/10/12 22:11:33 WARN DAGScheduler: Broadcasting large task binary with size 9.5 MiB\n",
      "22/10/12 22:11:35 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/10/12 22:11:39 WARN DAGScheduler: Broadcasting large task binary with size 7.5 MiB\n",
      "22/10/12 22:11:41 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/10/12 22:11:45 WARN DAGScheduler: Broadcasting large task binary with size 10.6 MiB\n",
      "22/10/12 22:11:47 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/10/12 22:11:51 WARN DAGScheduler: Broadcasting large task binary with size 10.4 MiB\n",
      "22/10/12 22:11:52 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/10/12 22:11:56 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "22/10/12 22:11:57 WARN DAGScheduler: Broadcasting large task binary with size 1427.7 KiB\n",
      "22/10/12 22:12:00 WARN DAGScheduler: Broadcasting large task binary with size 8.1 MiB\n",
      "22/10/12 22:12:01 WARN DAGScheduler: Broadcasting large task binary with size 1796.0 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train the models (it's three models, so it might take some time).\n",
    "dtc_model = dtc.fit(train_data)\n",
    "rfc_model = rfc.fit(train_data)\n",
    "#gbt_model = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d59a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison - compare each of these models\n",
    "dtc_predictions = dtc_model.transform(test_data)\n",
    "rfc_predictions = rfc_model.transform(test_data)\n",
    "# gbt_predictions = gbt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "344519be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the evaluator.\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b28fde2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error. \n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"Medical Care Expenditure Label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99ffdfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/12 22:12:06 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/10/12 22:12:10 WARN DAGScheduler: Broadcasting large task binary with size 48.6 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dtc_acc = acc_evaluator.evaluate(dtc_predictions)\n",
    "rfc_acc = acc_evaluator.evaluate(rfc_predictions)\n",
    "#gbt_acc = acc_evaluator.evaluate(gbt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce2f4d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the results!\n",
      "----------------------------------------\n",
      "A single decision tree has an accuracy of: 4.30%\n",
      "----------------------------------------\n",
      "A random forest ensemble has an accuracy of: 4.37%\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Let's do something a bit more complex in terms of printing, just so it's formatted nicer. \n",
    "print(\"Here are the results!\")\n",
    "print('-'*40)\n",
    "print('A single decision tree has an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n",
    "print('-'*40)\n",
    "print('A random forest ensemble has an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
    "print('-'*40)\n",
    "#print('An ensemble using GBT has an accuracy of: {0:2.2f}%'.format(gbt_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3e109c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/12 22:12:30 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------------+--------------------+--------------------+----------+\n",
      "|            features|Medical Care Expenditure Label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+------------------------------+--------------------+--------------------+----------+\n",
      "|(28,[0,1,2,3,4,5,...|                        6007.0|[149.0,20.0,36.0,...|[0.02039698836413...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                         605.0|[149.0,20.0,36.0,...|[0.02039698836413...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                         423.0|[149.0,20.0,36.0,...|[0.02039698836413...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                        3389.0|[149.0,20.0,36.0,...|[0.02039698836413...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                        1901.0|[149.0,20.0,36.0,...|[0.02039698836413...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                         145.0|[103.0,17.0,15.0,...|[0.02574356410897...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                           0.0|[149.0,20.0,36.0,...|[0.02039698836413...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                        4501.0|[103.0,17.0,15.0,...|[0.02574356410897...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                        2123.0|[149.0,20.0,36.0,...|[0.02039698836413...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                         798.0|[149.0,20.0,36.0,...|[0.02039698836413...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                        3519.0|[149.0,20.0,36.0,...|[0.02039698836413...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                        4441.0|[103.0,17.0,15.0,...|[0.02574356410897...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                        1757.0|[149.0,20.0,36.0,...|[0.02039698836413...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                        4268.0|[149.0,20.0,36.0,...|[0.02039698836413...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                          67.0|[149.0,20.0,36.0,...|[0.02039698836413...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                           0.0|[149.0,20.0,36.0,...|[0.02039698836413...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                        4605.0|[149.0,20.0,36.0,...|[0.02039698836413...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                         980.0|[149.0,20.0,36.0,...|[0.02039698836413...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                          55.0|[149.0,20.0,36.0,...|[0.02039698836413...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                        5536.0|[149.0,20.0,36.0,...|[0.02039698836413...|       0.0|\n",
      "+--------------------+------------------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtc_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b343bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/12 22:12:32 WARN DAGScheduler: Broadcasting large task binary with size 48.6 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------------+--------------------+--------------------+----------+\n",
      "|            features|Medical Care Expenditure Label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+------------------------------+--------------------+--------------------+----------+\n",
      "|(28,[0,1,2,3,4,5,...|                        6007.0|[0.46237309767138...|[0.02311865488356...|    3329.0|\n",
      "|(28,[0,1,2,3,4,5,...|                         605.0|[0.52752883971803...|[0.02637644198590...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                         423.0|[0.48386699673955...|[0.02419334983697...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                        3389.0|[0.45471195318845...|[0.02273559765942...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                        1901.0|[0.52399960032532...|[0.02619998001626...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                         145.0|[0.51793493887487...|[0.02589674694374...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                           0.0|[0.42722704542514...|[0.02136135227125...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                        4501.0|[0.56873015252959...|[0.02843650762647...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                        2123.0|[0.54796021141342...|[0.02739801057067...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                         798.0|[0.56388089993164...|[0.02819404499658...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                        3519.0|[0.60398343985828...|[0.03019917199291...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                        4441.0|[0.70792859071865...|[0.03539642953593...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                        1757.0|[0.53602114249690...|[0.02680105712484...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                        4268.0|[0.58375591242601...|[0.02918779562130...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                          67.0|[0.55460086887491...|[0.02773004344374...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                           0.0|[0.51622503376337...|[0.02581125168816...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                        4605.0|[0.56327477100880...|[0.02816373855044...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                         980.0|[0.55126855680769...|[0.02756342784038...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                          55.0|[0.57658495974072...|[0.02882924798703...|       0.0|\n",
      "|(28,[0,1,2,3,4,5,...|                        5536.0|[0.56168101628107...|[0.02808405081405...|       0.0|\n",
      "+--------------------+------------------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b079b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-run\n",
    "def reRunTreesClassifiers(final_data):\n",
    "    # split the data\n",
    "    train_data,test_data = final_data.randomSplit([0.7,0.3])\n",
    "    # load the models\n",
    "    dtc = DecisionTreeClassifier(labelCol='Medical Care Expenditure Label',featuresCol='features', maxBins=46)\n",
    "    rfc = RandomForestClassifier(labelCol='Medical Care Expenditure Label',featuresCol='features',  maxBins=46)\n",
    "    # train the data\n",
    "    dtc_model = dtc.fit(train_data)\n",
    "    rfc_model = rfc.fit(train_data)\n",
    "    # predict \n",
    "    dtc_predictions = dtc_model.transform(test_data)\n",
    "    rfc_predictions = rfc_model.transform(test_data)\n",
    "    # measure accuracy\n",
    "    dtc_acc = acc_evaluator.evaluate(dtc_predictions)\n",
    "    rfc_acc = acc_evaluator.evaluate(rfc_predictions)\n",
    "    print(\"Here are the results!\")\n",
    "    print('-'*40)\n",
    "    print('A single decision tree has an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n",
    "    print('-'*40)\n",
    "    print('A random forest ensemble has an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5fa7a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/12 23:10:01 WARN DAGScheduler: Broadcasting large task binary with size 1564.6 KiB\n",
      "22/10/12 23:10:07 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "22/10/12 23:10:12 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "22/10/12 23:10:24 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "22/10/12 23:10:26 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/10/12 23:10:29 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "22/10/12 23:10:31 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/10/12 23:10:35 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "22/10/12 23:10:37 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/10/12 23:10:41 WARN DAGScheduler: Broadcasting large task binary with size 7.2 MiB\n",
      "22/10/12 23:10:43 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/10/12 23:10:47 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "22/10/12 23:10:49 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "22/10/12 23:10:53 WARN DAGScheduler: Broadcasting large task binary with size 11.1 MiB\n",
      "22/10/12 23:10:55 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/10/12 23:10:59 WARN DAGScheduler: Broadcasting large task binary with size 7.7 MiB\n",
      "22/10/12 23:11:01 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/10/12 23:11:05 WARN DAGScheduler: Broadcasting large task binary with size 10.6 MiB\n",
      "22/10/12 23:11:07 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/10/12 23:11:11 WARN DAGScheduler: Broadcasting large task binary with size 11.1 MiB\n",
      "22/10/12 23:11:13 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/10/12 23:11:18 WARN DAGScheduler: Broadcasting large task binary with size 9.4 MiB\n",
      "22/10/12 23:11:20 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/10/12 23:11:25 WARN DAGScheduler: Broadcasting large task binary with size 10.2 MiB\n",
      "22/10/12 23:11:27 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/10/12 23:11:32 WARN DAGScheduler: Broadcasting large task binary with size 8.8 MiB\n",
      "22/10/12 23:11:33 WARN DAGScheduler: Broadcasting large task binary with size 1901.3 KiB\n",
      "22/10/12 23:11:37 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "22/10/12 23:11:38 WARN DAGScheduler: Broadcasting large task binary with size 1112.1 KiB\n",
      "22/10/12 23:11:42 WARN DAGScheduler: Broadcasting large task binary with size 1050.5 KiB\n",
      "22/10/12 23:11:46 WARN DAGScheduler: Broadcasting large task binary with size 46.0 MiB\n",
      "[Stage 171:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the results!\n",
      "----------------------------------------\n",
      "A single decision tree has an accuracy of: 4.05%\n",
      "----------------------------------------\n",
      "A random forest ensemble has an accuracy of: 4.05%\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "reRunTreesClassifiers(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35c23541",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# decrease features even more\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m assembler \u001b[38;5;241m=\u001b[39m \u001b[43mVectorAssembler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputCols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTotal Household Income\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHousehold Head Sex Index\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHousehold Head Marital Status Index\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m output_less \u001b[38;5;241m=\u001b[39m assembler\u001b[38;5;241m.\u001b[39mtransform(newstats)\n\u001b[1;32m      9\u001b[0m labelIndexer \u001b[38;5;241m=\u001b[39m StringIndexer(inputCol \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMedical Care Expenditure\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMedical Care Expenditure Label\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/pyspark/__init__.py:114\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/pyspark/ml/feature.py:4231\u001b[0m, in \u001b[0;36mVectorAssembler.__init__\u001b[0;34m(self, inputCols, outputCol, handleInvalid)\u001b[0m\n\u001b[1;32m   4227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4228\u001b[0m \u001b[38;5;124;03m__init__(self, \\\\*, inputCols=None, outputCol=None, handleInvalid=\"error\")\u001b[39;00m\n\u001b[1;32m   4229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4230\u001b[0m \u001b[38;5;28msuper\u001b[39m(VectorAssembler, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m-> 4231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.ml.feature.VectorAssembler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(handleInvalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4233\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py:64\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     62\u001b[0m java_obj \u001b[38;5;241m=\u001b[39m _jvm()\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m java_class\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 64\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjava_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m java_obj(\u001b[38;5;241m*\u001b[39mjava_args)\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1709\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1709\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py:281\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py:288\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 288\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py:402\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 402\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# decrease features even more\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Total Household Income\", \n",
    "                \"Household Head Sex Index\",\n",
    "                \"Household Head Marital Status Index\"],\n",
    "    outputCol=\"features\")\n",
    "output_less = assembler.transform(newstats)\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol = \"Medical Care Expenditure\", outputCol = \"Medical Care Expenditure Label\")\n",
    "output_less = labelIndexer.fit(output_less).transform(output_less)\n",
    "final_data_less = output_less.select(\"features\",'Medical Care Expenditure Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f972cd03",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# rerun\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mreRunTreesClassifiers\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_less\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36mreRunTreesClassifiers\u001b[0;34m(final_data)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreRunTreesClassifiers\u001b[39m(final_data):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# split the data\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     train_data,test_data \u001b[38;5;241m=\u001b[39m \u001b[43mfinal_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandomSplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# load the models\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     dtc \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMedical Care Expenditure Label\u001b[39m\u001b[38;5;124m'\u001b[39m,featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, maxBins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m46\u001b[39m)\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py:1188\u001b[0m, in \u001b[0;36mDataFrame.randomSplit\u001b[0;34m(self, weights, seed)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights must be positive. Found weight value: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m w)\n\u001b[1;32m   1187\u001b[0m seed \u001b[38;5;241m=\u001b[39m seed \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, sys\u001b[38;5;241m.\u001b[39mmaxsize)\n\u001b[0;32m-> 1188\u001b[0m rdd_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mrandomSplit(\u001b[43m_to_list\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mint\u001b[39m(seed))\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [DataFrame(rdd, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx) \u001b[38;5;28;01mfor\u001b[39;00m rdd \u001b[38;5;129;01min\u001b[39;00m rdd_array]\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/column.py:74\u001b[0m, in \u001b[0;36m_to_list\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m converter:\n\u001b[1;32m     73\u001b[0m     cols \u001b[38;5;241m=\u001b[39m [converter(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241m.\u001b[39mtoList(cols)\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1709\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1709\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py:281\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py:288\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 288\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop2.7/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py:402\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 402\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# rerun\n",
    "reRunTreesClassifiers(output_less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a192de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
